{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@ Author: Zachary Deng\n",
    "@ Date: 2021/2/9\n",
    "@ Brief: 使用 Actor-Critic算法训练CartPole-v0\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for Actor\n",
    "GAMMA = 0.95\n",
    "LR = 0.01\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False # 关闭非确定性算法 / 防止自动找最适的算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PGNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 40)\n",
    "        self.fc2 = nn.Linear(40, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "            nn.init.constant_(m.bias.data, 0.01)\n",
    "            \n",
    "class Actor(object):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # 初始化状态空间和动作空间的维度\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        # init network parameters\n",
    "        self.network = PGNetwork(self.state_dim, self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=LR)\n",
    "        \n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        observation = torch.FloatTensor(observation).to(device)\n",
    "        network_output = self.network.forward(observation)\n",
    "        with torch.no_grad():\n",
    "            prob_weights = F.softmax(network_output, dim=0).cuda().data.cpu().numpy()\n",
    "        action = np.random.choice(range(prob_weights.shape[0]), p=prob_weights)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, td_error):\n",
    "        self.time_step += 1\n",
    "        # Step1 : 前向传递\n",
    "        softmax_input = self.network.forward(torch.FloatTensor(state).to(device)).unsqueeze(0)\n",
    "        action = torch.LongTensor([action]).to(device)\n",
    "        neg_log_prob = F.cross_entropy(input=softmax_input, target=action, reduction='none')\n",
    "        \n",
    "        # Step2 : 反向传播\n",
    "        # 这里需要最大化当前策略的价值，因此需要最大化neg_log_prob * td_error,即最小化-neg_log_prob * td_error\n",
    "        loss = neg_log_prob * td_error\n",
    "        self.optimizer.zero_grad() # 将梯度初始化为零（因为一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和）\n",
    "        loss.backward() #反向传播求梯度\n",
    "        self.optimizer.step() #更新所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters for Critic\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 40)\n",
    "        self.fc2 = nn.Linear(40, 1) # 这个地方和之前略有区别，输出不是动作维度，而是一维\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "            nn.init.constant_(m.bias.data, 0.01)\n",
    "            \n",
    "class Critic(object):\n",
    "    def __init__(self, env):\n",
    "        # 状态空间和动作空间的维度\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        # init network parameters\n",
    "        self.network = QNetwork(self.state_dim, self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss() #对于Critic本身的参数更新一般用均方误差损失函数来更新\n",
    "        \n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "\n",
    "    def target_Q_network(self, state, reward, next_state):\n",
    "        s, s_ = torch.FloatTensor(state).to(device), torch.FloatTensor(next_state).to(device)\n",
    "        #前向传播\n",
    "        v = self.network.forward(s)\n",
    "        v_ = self.network.forward(s_)\n",
    "        \n",
    "        #反向传播\n",
    "        loss_q = self.loss_func(reward + GAMMA * v_, v) \n",
    "        self.optimizer.zero_grad()\n",
    "        loss_q.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            td_error = reward + GAMMA * v_ - v #参考公式 td_error = r + GAMMA*V(s') - V(s)\n",
    "            \n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 50000\n",
    "STEP = 3000\n",
    "TEST = 10\n",
    "\n",
    "def main():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    actor = Actor(env)\n",
    "    critic = Critic(env)\n",
    "    \n",
    "    for episode in range(EPISODE):\n",
    "        # initialize task\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Train\n",
    "        for step in range(STEP):\n",
    "            action = actor.choose_action(state) # softmax概率选择action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td_error = critic.target_Q_network(state, reward, next_state) # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "            actor.learn(state, action, td_error) # true_gradient = grad[logPi(s,a) * td_error]\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Test every 100 episodes\n",
    "        if episode%100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(TEST):\n",
    "                state = env.reset()\n",
    "                for j in range(STEP):\n",
    "                    env.render()\n",
    "                    action = actor.choose_action(state)\n",
    "                    state, reward, done, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "            ave_reward = total_reward / TEST\n",
    "            print('episode:', episode, 'Evaluation Average Reward:', ave_reward)\n",
    "            if ave_reward>=300:\n",
    "                break\n",
    "                \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    time_start = time.time()\n",
    "    main()\n",
    "    time_end = time.time()\n",
    "    print('Total time:', int(time_end-time_start), 's')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
